{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "from specaugment import spec_augment\n",
    "from torchvision import transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resizing the images and turning them to tensors\n",
    "transform1 = T.Compose([\n",
    "    T.Resize(224),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "#turning the 1 channel image to a 3 channel image and normalizing the image     \n",
    "transform2 = T.Compose([\n",
    "    T.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_dataset(Dataset):\n",
    "    def __init__(self, file_path, labels, for_training,transform1=None,transform2=None):\n",
    "        #self.df = pd.read_csv(labels)\n",
    "        self.df = labels\n",
    "        self.file_path = file_path\n",
    "        self.for_training = for_training\n",
    "        self.transform1 = transform1\n",
    "        self.transform2 = transform2\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = (self.df.iloc[index, 1])\n",
    "        file_data = Image.open(self.file_path + str(self.df[\"fname\"][index]))\n",
    "        file_data = self.transform1(file_data)\n",
    "        file_data = self.SpecAugment(file_data)\n",
    "        file_data = self.transform2(file_data)\n",
    "        \n",
    "        return file_data, label\n",
    "\n",
    "    #SpecAugment: apply image warping, frequency mask and time mask to images\n",
    "    #Is applied 20% of the time \n",
    "    def SpecAugment(self, data):\n",
    "        if self.for_training:\n",
    "            if random.random() < 0.2:\n",
    "                data = spec_augment(mel_spectrogram=data, time_warping_para=3,time_masking_para=5, frequency_masking_para=8)\n",
    "        return data\n",
    "    #Mix up: take a random image from the dataset, choose a random interpolation value and merge the images\n",
    "    #Is applied 20% of the time \n",
    "    def mixup(self, img, label):\n",
    "    if self.for_training:\n",
    "        if random.random() < 0.2:\n",
    "            random_index = random.randint(0,(self.__len__())-1)\n",
    "            label2 = (self.df.iloc[random_index, 1])\n",
    "            img2 = Image.open(self.file_path + str(self.df[\"fname\"][random_index]))\n",
    "            img2 = self.transform1(img2)\n",
    "\n",
    "            target1 = torch.zeros(7)\n",
    "            target2 = torch.zeros(7)\n",
    "\n",
    "            target1[label] = 1\n",
    "            target2[label2] = 1\n",
    "\n",
    "            #Mixup the images accordingly\n",
    "            alpha = 1.0\n",
    "            beta = 1.0\n",
    "\n",
    "            lam = np.random.beta(alpha, beta)\n",
    "            img = lam * img + (1 - lam) * img2\n",
    "            label = lam * target1 + (1 - lam) * target2\n",
    "            label = (label.argmax().item())\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dataloader(file_path, label_path, for_training, batch_size):\n",
    "\n",
    "    train_dataset = get_dataset(file_path,label_path, for_training,transform1,transform2)\n",
    "\n",
    "    return DataLoader(train_dataset, batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_dataloader(file_path, label_path, for_training, batch_size):\n",
    "\n",
    "    valid_dataset = get_dataset(file_path,label_path, for_training,transform1,transform2)\n",
    "\n",
    "    return DataLoader(valid_dataset, batch_size=batch_size,shuffle=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
